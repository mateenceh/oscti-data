<!DOCTYPE html>
<html lang="en-US">

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
	<title>Algorithmic Bias - Schneier on Security</title>
	<meta name='robots' content='max-image-preview:large' />
<link rel='dns-prefetch' href='//s.w.org' />
<link rel='dns-prefetch' href='//c0.wp.com' />
<link rel="alternate" type="application/rss+xml" title="Schneier on Security &raquo; Feed" href="https://www.schneier.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Schneier on Security &raquo; Comments Feed" href="https://www.schneier.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Schneier on Security &raquo; Algorithmic Bias Comments Feed" href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/feed/" />
<link rel='stylesheet' id='wp-block-library-css'  href='https://c0.wp.com/c/5.7/wp-includes/css/dist/block-library/style.min.css' type='text/css' media='all' />
<style id='wp-block-library-inline-css' type='text/css'>
.has-text-align-justify{text-align:justify;}
</style>
<style id='woocommerce-inline-inline-css' type='text/css'>
.woocommerce form .form-row .required { visibility: visible; }
</style>
<link rel='stylesheet' id='schneier-css'  href='https://149400697.v2.pressablecdn.com/wp-content/themes/schneier/style.css?ver=1.0.0' type='text/css' media='all' />
<link rel='stylesheet' id='schneier-main-css'  href='https://149400697.v2.pressablecdn.com/wp-content/themes/schneier/assets/dist/css/style.css?ver=1.0.2' type='text/css' media='all' />
<link rel='stylesheet' id='jetpack_css-css'  href='https://c0.wp.com/p/jetpack/9.5.2/css/jetpack.css' type='text/css' media='all' />
<script type='text/javascript' src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js?ver=3.5.1' id='jquery-js'></script>
<link rel="https://api.w.org/" href="https://www.schneier.com/wp-json/" /><link rel="alternate" type="application/json" href="https://www.schneier.com/wp-json/wp/v2/posts/7209" /><link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://www.schneier.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://www.schneier.com/wp-includes/wlwmanifest.xml" /> 

<link rel="canonical" href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html" />
<link rel='shortlink' href='https://www.schneier.com/?p=7209' />
<link rel="alternate" type="application/json+oembed" href="https://www.schneier.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.schneier.com%2Fblog%2Farchives%2F2015%2F08%2Falgorithmic_bia.html" />
<link rel="alternate" type="text/xml+oembed" href="https://www.schneier.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.schneier.com%2Fblog%2Farchives%2F2015%2F08%2Falgorithmic_bia.html&#038;format=xml" />
	<noscript><style>.woocommerce-product-gallery{ opacity: 1 !important; }</style></noscript>
	<link rel="icon" href="https://149400697.v2.pressablecdn.com/wp-content/uploads/2020/06/cropped-favicon-1-32x32.png" sizes="32x32" />
<link rel="icon" href="https://149400697.v2.pressablecdn.com/wp-content/uploads/2020/06/cropped-favicon-1-192x192.png" sizes="192x192" />
<link rel="apple-touch-icon" href="https://149400697.v2.pressablecdn.com/wp-content/uploads/2020/06/cropped-favicon-1-180x180.png" />
<meta name="msapplication-TileImage" content="https://149400697.v2.pressablecdn.com/wp-content/uploads/2020/06/cropped-favicon-1-270x270.png" />
		<style type="text/css" id="wp-custom-css">
			#schneier_promotion-2 img {
    max-width: 180px;
}		</style>
		</head>

<body class="post-template-default single single-post postid-7209 single-format-standard theme-schneier woocommerce-no-js">

	<div id="wrapper">
		<div id="main">

			<header>
				<div id="header">
					<h1>
						<a href="https://www.schneier.com/" rel="home">
							Schneier on Security						</a>
					</h1>
				</div>
			</header>

			<nav>
				<div class="nav" id="header-nav">
					<div class="menu-main-menu-container"><ul id="menu-main-menu" class="menu"><li id="menu-item-50175" class="menu1 menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-50175 current-menu-item"><a href="https://www.schneier.com">Blog</a></li>
<li id="menu-item-50916" class="menu2 menu-item menu-item-type-post_type menu-item-object-page menu-item-50916"><a href="https://www.schneier.com/crypto-gram/">Newsletter</a></li>
<li id="menu-item-50166" class="menu3 menu-item menu-item-type-post_type menu-item-object-page menu-item-50166"><a href="https://www.schneier.com/books/">Books</a></li>
<li id="menu-item-50169" class="menu4 menu-item menu-item-type-custom menu-item-object-custom menu-item-50169"><a href="https://www.schneier.com/essays/">Essays</a></li>
<li id="menu-item-50170" class="menu5 menu-item menu-item-type-custom menu-item-object-custom menu-item-50170"><a href="https://www.schneier.com/news/">News</a></li>
<li id="menu-item-50171" class="menu6 menu-item menu-item-type-custom menu-item-object-custom menu-item-50171"><a href="https://www.schneier.com/talks/">Talks</a></li>
<li id="menu-item-50167" class="menu7 menu-item menu-item-type-post_type menu-item-object-page menu-item-50167"><a href="https://www.schneier.com/academic/">Academic</a></li>
<li id="menu-item-50174" class="menu8 menu-item menu-item-type-post_type menu-item-object-page menu-item-50174"><a href="https://www.schneier.com/blog/about/">About Me</a></li>
</ul></div>				</div>
			</nav>

			
<aside>
	<div class="sidebar" id="sidebar-one">
		<section><div class="sidesection widget widget_schneier_search" id="schneier_search-3"><h3>Search</h3>
<p class="small">
	<em>Powered by <a href="https://duckduckgo.com/">DuckDuckGo</a></em></p>

<form method="get" action="https://duckduckgo.com/">

	<input type="hidden" name="kh" value="1" /><!-- use https -->

	<input id="search" name="q" size="15" maxlength="255" />

	<input type="submit" value="Go" /><br>

	<input type="radio" name="sites" id="searchblog" value="www.schneier.com/blog" />
	<label for="searchblog">Blog</label>

	<input type="radio" name="sites" id="searchessays" value="www.schneier.com/essays" />
	<label for="searchessays">Essays</label>

	<input type="radio" name="sites" id="searchall" value="www.schneier.com" checked="" />
	<label for="searchall">Whole site</label>

</form>
</div></section><section><div class="sidesection widget widget_schneier_social" id="schneier_social-2"><h3>Subscribe</h3>
<div id="subscription-buttons">

	<a href="https://www.schneier.com/feed/atom"><img src="https://149400697.v2.pressablecdn.com/wp-content/uploads/2019/10/rss-32px.png" alt="Atom Feed" /></a><a href="https://www.facebook.com/bruce.schneier"><img src="https://149400697.v2.pressablecdn.com/wp-content/uploads/2019/10/facebook-32px.png" alt="Facebook" /></a><a href="https://twitter.com/schneierblog/"><img src="https://149400697.v2.pressablecdn.com/wp-content/uploads/2019/10/twitter-32px.png" alt="Twitter" /></a><a href="https://www.amazon.com/Schneier-on-Security/dp/B0053HDDWW/"><img src="https://149400697.v2.pressablecdn.com/wp-content/uploads/2019/10/kindle-32px.png" alt="Kindle" /></a><a href="https://www.schneier.com/crypto-gram"><img src="https://149400697.v2.pressablecdn.com/wp-content/uploads/2019/10/email-32px.png" alt="E-Mail Newsletter (Crypto-Gram)" /></a>
</div>
</div></section>	</div>
</aside>

			<div id="content">

				
		<p id="breadcrumbs">

			<a href="https://www.schneier.com">Home</a><a href="https://www.schneier.com/blog/archives/">Blog</a>		</p>

		
<article id="post-7209" class="post-7209 post type-post status-publish format-standard hentry category-uncategorized tag-algorithms tag-bias">

	<div class="article">

		<h2 class="entry">Algorithmic Bias</h2>
		<p>Good <a href="http://www.nytimes.com/2015/08/11/upshot/algorithms-and-bias-q-and-a-with-cynthia-dwork.html">Q&#038;A</a> with Cynthia Dwork on algorithmic bias.</p>

		
			<p class="entry-tags">
				<span class="tags-links">Tags: <a href="https://www.schneier.com/tag/algorithms/" rel="tag">algorithms</a>, <a href="https://www.schneier.com/tag/bias/" rel="tag">bias</a></span>			</p>

		
		
		<p class="posted">
			<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html" rel="bookmark">Posted on August 14, 2015 at 8:20 AM</a>			•
			<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html#comments">26 Comments</a>		</p>

		<aside><div class="schneier-share share" data-uri="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html" data-title="Algorithmic Bias" data-order="facebook twitter tumblr" data-social-share-privacy="true" /></aside>
	</div>

</article>


	<h3 id="comments">Comments</h3>

	
		<article class="comment even thread-even depth-1" id="comment-254640">

			<div class="comment by-clive-robinson ">

				<p class="commentcredit">

					<span class="commenter">Clive Robinson</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254640">
						August 14, 2015 8:54 AM					</a>

				</p>

				<p>@ Bruce,</p>
<p>It&#8217;s a NY Times &#8220;pay walled&#8221; article.</p>
<p>Some of us have ethical and moral objections to handing over anything that will bring profit to it and it&#8217;s parent organization which shows every sign of being a criminal enterprise.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment odd alt thread-odd thread-alt depth-1" id="comment-254641">

			<div class="comment by-paul ">

				<p class="commentcredit">

					<span class="commenter">Paul</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254641">
						August 14, 2015 8:59 AM					</a>

				</p>

				<p>@Clive<br />
Google the headline and click the link to the site &#8211; you can usually then read the article for free. This works with most newspaper paywalls</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment even thread-even depth-1" id="comment-254642">

			<div class="comment by-ianf ">

				<p class="commentcredit">

					<span class="commenter">ianf</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254642">
						August 14, 2015 9:32 AM					</a>

				</p>

				<p>@Paul<br />
in hypertextual terms, that&#8217;s <a href="https://www.google.com/search?q=%22Algorithms+and+Bias:+Q.+and+A.+With+Cynthia+Dwork%22+site:nytimes.com&amp;hl=en&amp;source=lnms&amp;tbm=nws&amp;sa=X" rel="nofollow ugc">Google the headline and click the link to the site</a>, but, whatever the access method (from Europe), it is no longer &#8220;free&#8221; via Google News above a certain monthly NYT article limit (it doesn&#8217;t disclose the number, but sets a cumulative cookie on every read).</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment odd alt thread-odd thread-alt depth-1" id="comment-254643">

			<div class="comment by-buck ">

				<p class="commentcredit">

					<span class="commenter">Buck</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254643">
						August 14, 2015 9:55 AM					</a>

				</p>

				<p>You can clear your cookies first, but you must accept the cookie sent from NYT or you&#8217;ll be redirected to a paywall screen.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment even thread-even depth-1" id="comment-254644">

			<div class="comment by-me ">

				<p class="commentcredit">

					<span class="commenter">Me</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254644">
						August 14, 2015 10:30 AM					</a>

				</p>

				<p>@ianf</p>
<p>This is where CookieMonster comes in handy, either preventing the setting of said cookie, or removing it when the browser closes are great options.</p>
<p>I almost never allow persistent cookies any more.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment odd alt thread-odd thread-alt depth-1" id="comment-254645">

			<div class="comment by-carl-sai-mitchell ">

				<p class="commentcredit">

					<span class="commenter">Carl &#039;SAI&#039; Mitchell</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254645">
						August 14, 2015 10:46 AM					</a>

				</p>

				<p>@ianf, @me</p>
<p>Or Self-Destructing Cookies. It clears all cookies set by a site when you close the browser tab by default, though this can be overridden.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment even thread-even depth-1" id="comment-254646">

			<div class="comment by-edge ">

				<p class="commentcredit">

					<span class="commenter">edge</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254646">
						August 14, 2015 10:59 AM					</a>

				</p>

				<p>It seems that the simple 2-second answer is to just remove the variables that you want the algorithm to be blind to. The algorithms can&#8217;t make judgments on inputs that it doesn&#8217;t have access to.</p>
<p>(I suppose that there&#8217;s some complication in the fact that the remaining variables may not be completely independent from the ones you want to hide (e.g. zip code may be correlated to wealth or race).</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment odd alt thread-odd thread-alt depth-1" id="comment-254647">

			<div class="comment by-mrc ">

				<p class="commentcredit">

					<span class="commenter">MrC</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254647">
						August 14, 2015 11:12 AM					</a>

				</p>

				<p>@Clive:</p>
<p>In addition to providing some security and privacy protection against the background radiation of the web, NoScript + Self Destructing Cookies incidentally bypass NYT&#8217;s paywall.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment even thread-even depth-1" id="comment-254648">

			<div class="comment by-iondream ">

				<p class="commentcredit">

					<span class="commenter">Iondream</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254648">
						August 14, 2015 11:18 AM					</a>

				</p>

				<p>Just block javascript for the site, and you can read it for free. You can do it in chrome by clicking the site icon and going to &#8220;site settings&#8221;</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment odd alt thread-odd thread-alt depth-1" id="comment-254649">

			<div class="comment by-jd ">

				<p class="commentcredit">

					<span class="commenter">JD</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254649">
						August 14, 2015 11:24 AM					</a>

				</p>

				<p>The number of times that the word &#8220;Fair[ness]&#8221; was used, makes me want to hurl.  Not everything can or should be fair.  As pointed out in the jobs example, there are plenty of reasons for why an ad may have been shown to men more frequently, some of them are bad, most aren&#8217;t.  We shouldn&#8217;t assume that just because something /could/ have been caused by a bad choice that it WAS a bad choice and attack it.  And frankly, if Google wants to discriminate in their ads for jobs or anything else, I really just don&#8217;t care &#8211; they are a private business, and discriminating ads aren&#8217;t something we should regulate.  If you are going to regulate it, then you better start regulating radio ads too, that overtly discriminate in some way (a great example is on a classic hip-hop station here in Houston there is an attorney that advertises and admits that he only runs ads on stations that play the kind of music he likes, and wants to serve those &#8220;men that still get their fade trimmed in a barbershop&#8221; and other overt hints at targeting only black customers. &#8211; which I think is insulting to those he is advertising to, but I think he should be free to do so.)</p>
<p>If monster or dice was only showing high paying job postings to women, then that would be an issue &#8211; which is really my point &#8211; not everything needs to be &#8220;fair&#8221;, not everyone gets a freaking trophy just for showing up.</p>
<p>OF COURSE data are influenced by the inputs, and if you are not careful, your biases will affect the inputs, but this is literally not new news.  This has been known within the scientific method for hundreds of years &#8211; generally that&#8217;s why we cite &#8220;controlled variables&#8221;, and why sometimes experiments conclude completely wrong answers, because of bias that wasn&#8217;t seen going in.  Guess what, this is EXACTLY why you can get 2 different polls on the same issue to conclude conflicting answers &#8211; because of how you phrase the question (bias) and whom / how you target those you poll (bias).</p>
<p>IMHO, there is /some/ interesting things about bias in algorithms, but it&#8217;s not some panic level as the interviewer seems to portray.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment even thread-even depth-1" id="comment-254650">

			<div class="comment by-peter-pearson ">

				<p class="commentcredit">

					<span class="commenter">Peter Pearson</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254650">
						August 14, 2015 11:31 AM					</a>

				</p>

				<p>The only concrete example Ms. Dwork gives of an unbiased algorithm failing to be unbiased is the case of the unbiased algorithm being trained with biased data.  I&#8217;d suggest that to most people, &#8220;algorithm&#8221; refers to the trained algorithm; but in any event, you can avoid this confusion by insisting that both the algorithm and any training data be unbiased.  Ms. Dwork&#8217;s rush toward a solution based on government experts is unseemly.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment odd alt thread-odd thread-alt depth-1" id="comment-254651">

			<div class="comment by-rgaff ">

				<p class="commentcredit">

					<span class="commenter">rgaff</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254651">
						August 14, 2015 11:55 AM					</a>

				</p>

				<p>You know, guys, even just visiting the site is a kind of &#8220;support&#8221; in that you&#8217;re supporting them with your eyeballs.  So if you have an ethical and moral objection, you might consider staying away, regardless of whether the paywall can be defeated or not.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment even thread-even depth-1" id="comment-254652">

			<div class="comment by-a-mora ">

				<p class="commentcredit">

					<span class="commenter">A Mora</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254652">
						August 14, 2015 2:00 PM					</a>

				</p>

				<p>Edge, while what you suggest (removing variables) may work in some cases, in more complicated systems that can backfire. The author gave one example of this in her article, about admissions.</p>
<p>The trick is that fairness isn&#8217;t just about blind or indiscriminate application of  arbitrary rules. First you have to figure out what your objectives are, and what in your data set reflects that. In some cases accurately assessing someones performance requires context. For example, if you chart work place productivity, both drop like a rock around religious and national holidays.</p>
<p>As an example, I am granting a monthly bonus to the salesman for my software company. The US Divisions lead sales man was up 2% over last month and the Chinese is down 2%. So a blind system might just make a decision based on that, and miss the fact that the Chinese team was fighting a major headwind because it was Chinese New Year and the whole country shut down. By making the decision culturally aware you see that there was a huge flurry of activity in the previous month, followed by a drop as the whole country shut down. In this case, a salesman that was only 2% month on month would be off that chart if you graphed year on year. The same would have happened in the US, just a few months earlier.</p>
<p>So like so many things, you need to carefully review results to make sure they are accurate, and watch out for the law of unintended consequences.</p>
<p>Systems that are totally blind tend to hammer on some of the least deserving people. As an example college financial aid in the US is based on an &#8220;Expected Family Contribution&#8221; and applying for aid requires submitting not only your own but your &#8220;Parents&#8221; financial information. In years past this would deny or delay access to aid in the case of students who were estranged and supporting themselves. So &#8220;need&#8221; based aid was not delivered those most in need. Their system was designed to catch cheaters, and preformed well on the average, but failed to be &#8220;Fair&#8221; as a result.</p>
<p>Oh the joys of math in complex systems. In some ways I&#8217;d rather be assigned to crack AES256 messages by pen and paper then to be the one to design a &#8220;Fair&#8221; admissions algorithm. One problem is known and very hard. One is unknown and possibly impossible.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment odd alt thread-odd thread-alt depth-1" id="comment-254653">

			<div class="comment by-tyr ">

				<p class="commentcredit">

					<span class="commenter">tyr</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254653">
						August 14, 2015 4:23 PM					</a>

				</p>

				<p>You won&#8217;t find much blind faith here in the<br />
infallibility of computer outputs. People are<br />
burned far too often to be gullible about it.</p>
<p>That does not extend past the boundaries of<br />
the comp inner circle into the outside world.<br />
Pareto tried to put sociology on a scientific<br />
basis with limited success because of his way<br />
of describing what he saw in human behaviors.<br />
One thing he did notice was that a society<br />
able to allow circulation  (upward mmobility<br />
is the current buzzword) of the talented had<br />
a much better outcome in the long run. Using<br />
bias to reinforce the status quo is a really<br />
bad idea no matter where it comes from. If a<br />
programmer carries this bias in then you wind<br />
up wasting the most valuable of resources a<br />
society possesses without knowing why society<br />
is failing around you. Notice that this is not<br />
some sappy idea of levelling the playing field<br />
so that retards can feel good about their lack<br />
of accomplishments. Its about making sure there<br />
is a path for the excellent to rise to useful<br />
positions when they are capable of it. The<br />
shameful example of women in the sciences is<br />
clearly a result of bias being applied against<br />
people who were head and shoulders above their<br />
esteemed colleagues who used them to reap the<br />
fame of the womens efforts. The results have<br />
been to set human civilization back and hold<br />
it back to pander to inferior male egos.</p>
<p>I&#8217;m glad somebody has had the gall to point<br />
out that assumptions of what we are doing with<br />
comp algorythmns need to be checked now and<br />
then to make sure they are doing what is intended<br />
instead of just reinforcing retardation.</p>
<p>@Clive<br />
That men who stare comment was about the BBC4<br />
documentary not the hollywood fluff film.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment even thread-even depth-1" id="comment-254654">

			<div class="comment by-squid-friday ">

				<p class="commentcredit">

					<span class="commenter">Squid Friday</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254654">
						August 14, 2015 4:27 PM					</a>

				</p>

				<p><a href="http://www.wired.com/2015/08/absurd-creature-week-squid-looks-like-bee-stung-eyeball/" rel="nofollow ugc">http://www.wired.com/2015/08/absurd-creature-week-squid-looks-like-bee-stung-eyeball/</a></p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment odd alt thread-odd thread-alt depth-1" id="comment-254655">

			<div class="comment by-gerard-van-vooren ">

				<p class="commentcredit">

					<span class="commenter">Gerard van Vooren</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254655">
						August 14, 2015 4:46 PM					</a>

				</p>

				<p>@ tyr,</p>
<p>Your line width is getting smaller and smaller. There is no need to break the lines.</p>
<p>(offtopic note)</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment even thread-even depth-1" id="comment-254656">

			<div class="comment by-lessthanobvious ">

				<p class="commentcredit">

					<span class="commenter">LessThanObvious</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254656">
						August 14, 2015 5:11 PM					</a>

				</p>

				<p>I continue see algorithmic decision making as a very scary trend. There is so much reliance on what the computer decides. If we have to tweak the algorithm to protect minorities and women, then aren&#8217;t there likely a large number of other corner cases where the algorithm fails the fairness test on an individual basis? When it comes to things that affect people&#8217;s lives like lending, hiring and school admission I really recoil at the idea of such arbitrary selectors being used. Machine learning algorithms it seems often use criteria that are &#8220;good enough&#8221; correlation.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment odd alt thread-odd thread-alt depth-1" id="comment-254657">

			<div class="comment by-ray-dillinger ">

				<p class="commentcredit">

					<span class="commenter">Ray Dillinger</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254657">
						August 14, 2015 5:55 PM					</a>

				</p>

				<p>Way back in the wayback, in school, I created an expert system that was supposed to simulate rational executive decision-making.  One that would decide everything based on merit and legitimate business objectives.  Completely fair, right?  And so it was in the beginning.</p>
<p>But when its available information included the fact that women in general faced pay discrimination, it immediately lowered the salaries of all its female &#8220;employees&#8221; by the same percentage &#8211; on the grounds that they would accept less pay because they weren&#8217;t getting a better offer elsewhere.  Perfectly rational.  Perfectly unbiased.  Perfectly logical response to human bigotry.  And a big honking signal that mere rationality won&#8217;t solve the problem.</p>
<p>This sort of thing continues the trend&#8230;.  Obviously ethnic names elicit different clickbait ads based on the stereotypes about their ethnicity, not because the algorithm is bigoted, but because people are.  Those are the clickbait ads that generate the biggest revenue streams.</p>
<p>As long as people are bigoted, rational responses to (or exploitation of) people will remain bigoted.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment even thread-even depth-1" id="comment-254658">

			<div class="comment by-mike-barno ">

				<p class="commentcredit">

					<span class="commenter">Mike Barno</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254658">
						August 14, 2015 6:31 PM					</a>

				</p>

				<p>@ edge</p>
<p><cite><br />
(I suppose that there&#8217;s some complication in the fact that the remaining variables may not be completely independent from the ones you want to hide (e.g. zip code may be correlated to wealth or race).<br />
</cite></p>
<p>In the USA, ZIP Code is strongly correlated with both wealth and race.  The whole PRIZM segmentation system (from NPDC, and then from Claritas, before Nielsen wiped the Claritas name) was based on this originally &#8212; not race as a selector, but <em>income</em> as one of a few selectors for all the demographics (including <em>wealth</em> and <em>race</em>) that went with Census household data, and state and local data.  It got more granular when they went to the ZIP+4 level.  Later, PRIZM became based on direct household-level data from data brokers, with ZIP-based coding used only as a backup.  But selecting ZIPs with highest and lowest median household wealth shows that highs are far far above lows.  Selecting ZIPs by proportion of race shows the whitest and blackest and most-Asian ZIPs have vastly different proportions.</p>
<p>So if you run a pilot program in Arlington Heights, Virginia, and expect its results to be the same in Ferguson, Missouri, you might let incorrect assumptions cause a bad decision.</p>
<p>More broadly:<br />
If you overly rely on &#8220;hiding&#8221; variables either by refusing to consider them where attention might be needed, or by using a simple adjustment that assumes &#8220;all else being equal&#8221; relying on not-always-valid assumptions,.. then your model will draw conclusions that draw from your presumptions.  If you decide married people score 50 favorable points and unmarried people living together score 10, your algorithm will skew toward more-religious people, toward older people, toward more-traditional communities, differently than if all people sharing households got 30 points regardless of marital status.</p>
<p>I see concerns of biased algorithms all the time in matters that have become political footballs.  Climate-change study, of course, before it ever gets to the stage of assigning human causes.  Anything to do with Obamacare, especially Medicaid expansion.  Economic impact studies when the developer might be bribing the mayor and council or begging for tax breaks.  Government agency studies purporting to show their program&#8217;s effectiveness, with a funding vote coming up soon.</p>
<p>As others have noted, it gets a bunch more complex when you have a &#8220;learning&#8221; system.  Depending on what methodology and data are used for training, a system can learn that previously existing biases are the norm, the baseline.  If that gets treated as &#8220;these are the profitable customers we want to keep supporting&#8221;, the system becomes an excuse justifying continued discrimination.  And once a system is built and running, even its managers might not realize what assumptions got trained into it.  The ordinary citizen, whose loan wasn&#8217;t approved or whose taxes were audited or whose communications were surveilled, would never know.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment odd alt thread-odd thread-alt depth-1" id="comment-254659">

			<div class="comment by-delaboetie ">

				<p class="commentcredit">

					<span class="commenter">deLaBoetie</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254659">
						August 15, 2015 7:30 AM					</a>

				</p>

				<p>This focus on bias is &#8211; I think &#8211; a small part of something that worries me far more, which is false positives.  The data mining approach, even with careful &#8220;selectors&#8221;, is demonstrably disastrous from the point of view of both the probability and the scale of those false positives.  This has been confirmed by the agencies themselves.</p>
<p>And the issue with that, leaving aside the wasted time of the agencies, is that I believe those false positives from indiscriminate selectors (whether biased or not), will effectively result in automated targeting.  It will have the effect of being targeted, because before you know it, as an innocent false positive, you will be attacked with algorithmic scripts, either by auto-infecting your systems, or putting you on no-fly lists etc. with no human involvement at all, and no effective remedy or compensation.  Because it&#8217;s technically possible, and easier to do the scatter-gun approach, and because they do not bear the costs, experience has shown that this is exactly what will happen.</p>
<p>From the agencies point of view, as well as the haystack problem of false positives, the bias issue will inevitably result in false negatives too &#8211; my guess is that many &#8220;baddies&#8221; would slip through in exactly the same way as they would with biased border control inspections.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment even thread-even depth-1" id="comment-254660">

			<div class="comment by-winter ">

				<p class="commentcredit">

					<span class="commenter">winter</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254660">
						August 15, 2015 8:46 AM					</a>

				</p>

				<p>Isn&#8217;t this just another example of machines amplifying human behavior?</p>
<p>In this case, they will amplify all our biases.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment byuser comment-author-schneier odd alt thread-odd thread-alt depth-1" id="comment-254661">

			<div class="comment by-bruce-schneier official">

				<p class="commentcredit">

					<span class="commenter">Bruce Schneier</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254661">
						August 15, 2015 2:48 PM					</a>

				</p>

				<p>&#8220;It&#8217;s a NY Times &#8220;pay walled&#8221; article.&#8221;</p>
<p>Not on my computer or network. Odd.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment even thread-even depth-1" id="comment-254662">

			<div class="comment by-for-those-without-access ">

				<p class="commentcredit">

					<span class="commenter">For those without access</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254662">
						August 15, 2015 4:01 PM					</a>

				</p>

				<p>Algorithms and Bias: Q. and A. With Cynthia Dwork</p>
<p>Algorithms have become one of the most powerful arbiters in our lives. They make decisions about the news we read, the jobs we get, the people we meet, the schools we attend and the ads we see.</p>
<p>Yet there is growing evidence that algorithms and other types of software can discriminate. The people who write them incorporate their biases, and algorithms often learn from human behavior, so they reflect the biases we hold. For instance, research has shown that ad-targeting algorithms have shown ads for high-paying jobs to men but not women, and ads for high-interest loans to people in low-income neighborhoods.</p>
<p>Cynthia Dwork, a computer scientist at Microsoft Research in Silicon Valley, is one of the leading thinkers on these issues. In an Upshot interview, which has been edited, she discussed how algorithms learn to discriminate, who’s responsible when they do, and the trade-offs between fairness and privacy.<br />
Continue reading the main story</p>
<p>Q: Some people have argued that algorithms eliminate discrimination because they make decisions based on data, free of human bias. Others say algorithms reflect and perpetuate human biases. What do you think?</p>
<p>A: Algorithms do not automatically eliminate bias. Suppose a university, with admission and rejection records dating back for decades and faced with growing numbers of applicants, decides to use a machine learning algorithm that, using the historical records, identifies candidates who are more likely to be admitted. Historical biases in the training data will be learned by the algorithm, and past discrimination will lead to future discrimination.</p>
<p>Q: Are there examples of that happening?</p>
<p>A: A famous example of a system that has wrestled with bias is the resident matching program that matches graduating medical students with residency programs at hospitals. The matching could be slanted to maximize the happiness of the residency programs, or to maximize the happiness of the medical students. Prior to 1997, the match was mostly about the happiness of the programs.</p>
<p>This changed in 1997 in response to “a crisis of confidence concerning whether the matching algorithm was unreasonably favorable to employers at the expense of applicants, and whether applicants could ‘game the system,’ ” according to a paper by Alvin Roth and Elliott Peranson published in The American Economic Review.</p>
<p>Q: You have studied both privacy and algorithm design, and co-wrote a paper, “Fairness Through Awareness,” that came to some surprising conclusions about discriminatory algorithms and people’s privacy. Could you summarize those?</p>
<p>A: “Fairness Through Awareness” makes the observation that sometimes, in order to be fair, it is important to make use of sensitive information while carrying out the classification task. This may be a little counterintuitive: The instinct might be to hide information that could be the basis of discrimination.</p>
<p>Q: What’s an example?</p>
<p>A: Suppose we have a minority group in which bright students are steered toward studying math, and suppose that in the majority group bright students are steered instead toward finance. An easy way to find good students is to look for students studying finance, and if the minority is small, this simple classification scheme could find most of the bright students.</p>
<p>But not only is it unfair to the bright students in the minority group, it is also low utility. Now, for the purposes of finding bright students, cultural awareness tells us that “minority+math” is similar to “majority+finance.” A classification algorithm that has this sort of cultural awareness is both more fair and more useful.</p>
<p>Fairness means that similar people are treated similarly. A true understanding of who should be considered similar for a particular classification task requires knowledge of sensitive attributes, and removing those attributes from consideration can introduce unfairness and harm utility.</p>
<p>Q: How could the university create a fairer algorithm? Would it mean more human involvement in the work that software does, collecting more personal data from students or taking a different approach when the algorithm is being created?</p>
<p>A: It would require serious thought about who should be treated similarly to whom. I don’t know of any magic bullets, and it is a fascinating question whether it is possible to use techniques from machine learning to help figure this out. There is some preliminary work on this problem, but this direction of research is still in its infancy.</p>
<p>Q: Another recent example of the problem came from Carnegie Mellon University, where researchers found that Google’s advertising system showed an ad for a career coaching service for “$200k+” executive jobs to men much more often than to women. What did that study tell us about these issues?</p>
<p>A: The paper is very thought-provoking. The examples described in the paper raise questions about how things are done in practice. I am currently collaborating with the authors and others to consider the differing legal implications of several ways in which an advertising system could give rise to these behaviors.</p>
<p>Q: What are some of the ways it could have happened? It seems that the advertiser could have targeted men, or the algorithm determined that men were more likely to click on the ad.</p>
<p>A: Here is a different plausible explanation: It may be that there is more competition to advertise to women, and the ad was being outbid when the web surfer was female.</p>
<p>Q: The law protects certain groups from discrimination. Is it possible to teach an algorithm to do the same?</p>
<p>A: This is a relatively new problem area in computer science, and there are grounds for optimism — for example, resources from the Fairness, Accountability and Transparency in Machine Learning workshop, which considers the role that machines play in consequential decisions in areas like employment, health care and policing. This is an exciting and valuable area for research.</p>
<p>Q: Whose responsibility is it to ensure that algorithms or software are not discriminatory?</p>
<p>A: This is better answered by an ethicist. I’m interested in how theoretical computer science and other disciplines can contribute to an understanding of what might be viable options.</p>
<p>The goal of my work is to put fairness on a firm mathematical foundation, but even I have just begun to scratch the surface. This entails finding a mathematically rigorous definition of fairness and developing computational methods — algorithms — that guarantee fairness.</p>
<p>Q: In your paper on fairness, you wrote that ideally a regulatory body or civil rights organization would impose rules governing these issues. The tech world is notoriously resistant to regulation, but do you believe it might be necessary to ensure fairness in algorithms?</p>
<p>A: Yes, just as regulation currently plays a role in certain contexts, such as advertising jobs and extending credit.</p>
<p>Q: Should computer science education include lessons on how to be aware of these issues and the various approaches to addressing them?</p>
<p>A: Absolutely! First, students should learn that design choices in algorithms embody value judgments and therefore bias the way systems operate. They should also learn that these things are subtle: For example, designing an algorithm for targeted advertising that is gender-neutral is more complicated than simply ensuring that gender is ignored. They need to understand that classification rules obtained by machine learning are not immune from bias, especially when historical data incorporates bias. Techniques for addressing these kinds of issues should be quickly incorporated into curricula as they are developed.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment odd alt thread-odd thread-alt depth-1" id="comment-254663">

			<div class="comment by-futureskynetdeveloper ">

				<p class="commentcredit">

					<span class="commenter">futureskynetdeveloper</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254663">
						August 16, 2015 9:00 PM					</a>

				</p>

				<p>I do not think biases are a particularly bad thing. The developer is only creating the algorithm to the clients specification, in the end the developer usually is just doing it for the monetary compensation, not to please the inner workings of society.</p>
<p>The client determines to hire the developer to create the biased algorithm. Generally to meet a specific business need such as @JD pointed out. In that case the Attorney is probably sitting on a gold mine by focusing on just black males who like rap music from that area, data probably shows there is a lot of demand in that area for that service from that group of people.</p>
<p>I like to judge algorithms for their effectiveness, how well they met the scope of the project. Even if the scope for the project was deciding &#8220;which babies should be thrown out the garbage shoot at a hospital and which babies should be delivered to the nursery&#8221; there still would be a final grade on how well that algorithm met its objectives.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment even thread-even depth-1" id="comment-254664">

			<div class="comment by-spectacular ">

				<p class="commentcredit">

					<span class="commenter">spectacular</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254664">
						August 16, 2015 9:50 PM					</a>

				</p>

				<p>@ futureskynetdev, those without access</p>
<blockquote>
<blockquote><p>
    Algorithms have become one of the most powerful arbiters in our lives.
  </p></blockquote>
</blockquote>
<p>When algorithm is used in decisions at scale, it is shaping, imho. As all things are relative, algorithms can predict, as well as shape. Tweaking is imperative as it require a feedback process. Thus everything has a cyclical tendency.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

		<article class="comment odd alt thread-odd thread-alt depth-1" id="comment-254665">

			<div class="comment by-nate ">

				<p class="commentcredit">

					<span class="commenter">Nate</span> •

					<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/#comment-254665">
						August 17, 2015 8:25 AM					</a>

				</p>

				<p>Edge wrote:</p>
<blockquote><p>
  It seems that the simple 2-second answer is to just remove the variables that you want the<br />
  algorithm to be blind to. The algorithms can&#8217;t make judgments on inputs that it doesn&#8217;t have<br />
  access to.
</p></blockquote>
<p>As you (and others) point out, it&#8217;s very difficult to control for undesired factors in real world data.</p>
<p>Something that hasn&#8217;t been mentioned by others is that, with modern information technology, it can be quite easy to construct a &#8216;stealth selector&#8217; that deliberately creates a bias in a selection, but uses santized criteria.   So, for example, if a credit bureau wanted to build a &#8216;jim crow&#8217; credit rating while retaining deniability about racial bias, they could probably do so with little trouble using the data they collect.</p>

				
			</div>

		</article>

		</li><!-- #comment-## -->

	<p class="subscribe-comments">
		<a href="https://www.schneier.com/blog/archives/2015/08/algorithmic_bia.html/feed/">
			<img alt="Atom Feed" src="https://149400697.v2.pressablecdn.com/wp-content/themes/schneier/assets/images/rss.png">
			Subscribe to comments on this entry		</a>
	</p>

		<div id="respond" class="comment-respond">
		<h2 class="comments-open-header">Leave a comment <small><a rel="nofollow" id="cancel-comment-reply-link" href="/blog/archives/2015/08/algorithmic_bia.html#respond" style="display:none;">Cancel reply</a></small></h2><form action="https://www.schneier.com/wp-comments-post.php" method="post" id="commentform" class="comment-form" novalidate><a href="https://www.schneier.com/wp-login.php?redirect_to=https%3A%2F%2Fwww.schneier.com%2Fblog%2Farchives%2F2015%2F08%2Falgorithmic_bia.html" title="Login">Login</a><p class="comment-form-author"><label for="author">Name</label> <input id="author" name="author" type="text" value="" size="30" maxlength="245" /></p>
<p class="comment-form-email"><label for="email">Email</label> <input id="email" name="email" type="email" value="" size="30" maxlength="100" /></p>
<p class="comment-form-url"><label for="url">URL:</label> <input id="url" name="url" type="url" value="" size="30" maxlength="200" /></p>
<p class="comment-form-cookies-consent"><input id="wp-comment-cookies-consent" name="wp-comment-cookies-consent" type="checkbox" value="yes" /> <label for="wp-comment-cookies-consent">Remember personal info?</label></p>

<p class="comment-form-author">

	<label for="comm_capt_challenge">
		Fill in the blank: the name of this blog is Schneier on ___________ (required):	</label>

	<input id="comm_capt_challenge" name="comm_capt_challenge" size="30" type="text" />
</p>

<div class="comment-form-comment">

	<label for="comment">Comments:</label>

	<textarea id="comment" name="comment" cols="45" rows="8" maxlength="65525" required="required"></textarea>

	<div id="preview-box" class="preview-box hide"></div>
	<img class="comment-loading hide" src="https://149400697.v2.pressablecdn.com/wp-content/themes/schneier/assets/images/loader.gif" />

</div>

<p id="allowed">

	<strong>Allowed HTML</strong>
	&lt;a href=&quot;URL&quot;&gt; &bull; &lt;em&gt; &lt;cite&gt; &lt;i&gt; &bull; &lt;strong&gt; &lt;b&gt; &bull; &lt;sub&gt; &lt;sup&gt; &bull; &lt;ul&gt; &lt;ol&gt; &lt;li&gt; &bull; &lt;blockquote&gt; &lt;pre&gt;
	<strong>Markdown Extra</strong> syntax via <a href="https://michelf.ca/projects/php-markdown/extra/">https://michelf.ca/projects/php-markdown/extra/</a>
</p>

<input type="hidden" id="wp_comment_nonce" name="wp_comment_nonce" value="4b1534b792" /><input type="hidden" name="_wp_http_referer" value="/blog/archives/2015/08/algorithmic_bia.html" />
<input type="button" id="comment-preview" class="comment-preview comment-actions" value="Preview" />
<input type="button" id="comment-write" class="comment-write comment-actions hide" value="Edit" />

<p class="form-submit"><input name="submit" type="submit" id="submit" class="submit" value="Submit" /> <input type='hidden' name='comment_post_ID' value='7209' id='comment_post_ID' />
<input type='hidden' name='comment_parent' id='comment_parent' value='0' />
</p></form>	</div><!-- #respond -->
	
	<div class="stepthrough">
		<a href="https://www.schneier.com/blog/archives/2015/08/cryptography_fr.html" rel="prev">← Cryptography from the 13th Century</a>		<a href="https://www.schneier.com/blog/archives/2015/08/friday_squid_bl_491.html" rel="next">Friday Squid Blogging: Strawberry Squid →</a>	</div>

	
<p id="powered">Sidebar photo of Bruce Schneier by Joe MacInnis.</p>
		</div>

		
<aside>
	<div class="sidebar" id="sidebar-two">
		<section><div class="sidesection widget widget_schneier_about" id="schneier_about-2"><h3>About Bruce Schneier</h3><img src="https://149400697.v2.pressablecdn.com/wp-content/uploads/2019/10/Bruce-Schneier.jpg" /><p><p class="small">I am a <a href="https://public-interest-tech.com/">public-interest technologist</a>, working at the intersection of security, technology, and people. I've been writing about security issues on my <a href="/">blog</a> since 2004, and in my monthly <a href="/crypto-gram/">newsletter</a> since 1998. I'm a fellow and lecturer at Harvard's <a href="https://www.hks.harvard.edu/faculty/bruce-schneier">Kennedy School</a>, a board member of <a href="https://www.eff.org/">EFF</a>, and the Chief of Security Architecture at <a href="https://inrupt.com/">Inrupt, Inc.</a> This personal website expresses the opinions of none of those organizations.</p>
</p></div></section><section><div class="sidesection widget widget_schneier_related_posts" id="schneier_related_posts-2"><h3>Related Entries</h3>
<ul>

	<li><a href="https://www.schneier.com/blog/archives/2021/03/illegal-content-and-the-blockchain.html">Illegal Content and the Blockchain</a></li><li><a href="https://www.schneier.com/blog/archives/2021/03/national-security-risks-of-late-stage-capitalism.html">National Security Risks of Late-Stage Capitalism</a></li><li><a href="https://www.schneier.com/blog/archives/2021/02/weis-2021-call-for-papers.html">WEIS 2021 Call for Papers</a></li><li><a href="https://www.schneier.com/blog/archives/2021/02/ransomware-profitability.html">Ransomware Profitability</a></li><li><a href="https://www.schneier.com/blog/archives/2021/02/presidential-cybersecurity-and-pelotons.html">Presidential Cybersecurity and Pelotons</a></li><li><a href="https://www.schneier.com/blog/archives/2021/01/police-have-disrupted-the-emotet-botnet.html">Police Have Disrupted the Emotet Botnet</a></li>
</ul>
</div></section><section><div class="sidesection widget widget_schneier_featured_essays" id="schneier_featured_essays-2"><h3>Featured Essays</h3>
	<ul>
		<li><a href="https://www.schneier.com/essays/archives/2016/04/the_value_of_encrypt.html">The Value of Encryption</a></li><li><a href="https://www.schneier.com/essays/archives/2016/03/data_is_a_toxic_asse.html">Data Is a Toxic Asset, So Why Not Throw It Out?</a></li><li><a href="https://www.schneier.com/essays/archives/2014/01/how_the_nsa_threaten.html">How the NSA Threatens National Security</a></li><li><a href="https://www.schneier.com/essays/archives/2009/01/terrorists_may_use_g.html">Terrorists May Use Google Earth, But Fear Is No Reason to Ban It</a></li><li><a href="https://www.schneier.com/essays/archives/2007/01/in_praise_of_securit.html">In Praise of Security Theater</a></li><li><a href="https://www.schneier.com/essays/archives/2006/08/refuse_to_be_terrori.html">Refuse to be Terrorized</a></li><li><a href="https://www.schneier.com/essays/archives/2006/05/the_eternal_value_of.html">The Eternal Value of Privacy</a></li><li><a href="https://www.schneier.com/essays/archives/2005/09/terrorists_dont_do_m.html">Terrorists Don&#039;t Do Movie Plots</a></li>	</ul>

	<p><a href="https://www.schneier.com/essays/">More Essays</a></p></div></section><section><div class="sidesection widget widget_schneier_archives" id="schneier_archives-2"><h3>Blog Archives</h3>
<ul>

	<li><a href="https://www.schneier.com/blog/calendar.html/">Archive by Month</a></li><li><a href="https://www.schneier.com/blog/newcomments.html/">100 Latest Comments</a></li></ul>

<h4>Blog Tags</h4><ul class="top-tags"><li><a href="https://www.schneier.com/tag/3d-printers/">3d printers</a></li><li><a href="https://www.schneier.com/tag/9-11/">9/11</a></li><li><a href="https://www.schneier.com/tag/aaron-swartz/">Aaron Swartz</a></li><li><a href="https://www.schneier.com/tag/academic/">academic</a></li><li><a href="https://www.schneier.com/tag/academic-papers/">academic papers</a></li><li><a href="https://www.schneier.com/tag/accountability/">accountability</a></li><li><a href="https://www.schneier.com/tag/aclu/">ACLU</a></li><li><a href="https://www.schneier.com/tag/activism/">activism</a></li><li><a href="https://www.schneier.com/tag/adobe/">Adobe</a></li><li><a href="https://www.schneier.com/tag/advanced-persistent-threats/">advanced persistent threats</a></li><li><a href="https://www.schneier.com/tag/adware/">adware</a></li><li><a href="https://www.schneier.com/tag/aes/">AES</a></li><li><a href="https://www.schneier.com/tag/afghanistan/">Afghanistan</a></li><li><a href="https://www.schneier.com/tag/air-marshals/">air marshals</a></li><li><a href="https://www.schneier.com/tag/air-travel/">air travel</a></li><li><a href="https://www.schneier.com/tag/airgaps/">airgaps</a></li><li><a href="https://www.schneier.com/tag/al-qaeda/">al Qaeda</a></li><li><a href="https://www.schneier.com/tag/alarms/">alarms</a></li><li><a href="https://www.schneier.com/tag/algorithms/">algorithms</a></li><li><a href="https://www.schneier.com/tag/alibis/">alibis</a></li><li><a href="https://www.schneier.com/tag/amazon/">Amazon</a></li><li><a href="https://www.schneier.com/tag/android/">Android</a></li><li><a href="https://www.schneier.com/tag/anonymity/">anonymity</a></li><li><a href="https://www.schneier.com/tag/anonymous/">Anonymous</a></li><li><a href="https://www.schneier.com/tag/antivirus/">antivirus</a></li><li><a href="https://www.schneier.com/tag/apache/">Apache</a></li><li><a href="https://www.schneier.com/tag/apple/">Apple</a></li><li><a href="https://www.schneier.com/tag/applied-cryptography/">Applied Cryptography</a></li><li><a href="https://www.schneier.com/tag/artificial-intelligence/">artificial intelligence</a></li><li><a href="https://www.schneier.com/tag/assassinations/">assassinations</a></li></ul><p><a href="https://www.schneier.com/blog/tags.html/">More Tags</a></p></div></section><section><div class="sidesection widget widget_schneier_latest_book" id="schneier_latest_book-3"><h3>Latest Book</h3><a href="https://www.schneier.com/books/click-here/"><img class="sidepic" alt="Click Here to Kill Everybody" src="https://149400697.v2.pressablecdn.com/wp-content/uploads/2018/07/book-ch2-200w.png" /></a><p><a href="https://www.schneier.com/books/">More Books</a></p></div></section><section><div class="sidesection widget widget_schneier_promotion" id="schneier_promotion-2">
<a href="https://www.eff.org/issues/bloggers/legal/join">
	<img src="https://149400697.v2.pressablecdn.com/wp-content/themes/schneier/assets/images/join-eff@2x.png" id="effbutton" alt="Support Bloggers' Rights!" title="Support Bloggers' Rights!" /></a>

<a href="https://npo.networkforgood.org/Donate/Donate.aspx?npoSubscriptionId=8252">
	<img src="https://149400697.v2.pressablecdn.com/wp-content/themes/schneier/assets/images/support-epic@2x.png" alt="Defend Privacy--Support Epic" title="Defend Privacy--Support Epic" /></a>
</div></section>	</div>
</aside>

		<footer>
			<nav>
				<div class="nav" id="footer-nav">
					<div class="menu-main-menu-container"><ul id="menu-main-menu-1" class="menu"><li class="menu1 menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-50175 current-menu-item"><a href="https://www.schneier.com">Blog</a></li>
<li class="menu2 menu-item menu-item-type-post_type menu-item-object-page menu-item-50916"><a href="https://www.schneier.com/crypto-gram/">Newsletter</a></li>
<li class="menu3 menu-item menu-item-type-post_type menu-item-object-page menu-item-50166"><a href="https://www.schneier.com/books/">Books</a></li>
<li class="menu4 menu-item menu-item-type-custom menu-item-object-custom menu-item-50169"><a href="https://www.schneier.com/essays/">Essays</a></li>
<li class="menu5 menu-item menu-item-type-custom menu-item-object-custom menu-item-50170"><a href="https://www.schneier.com/news/">News</a></li>
<li class="menu6 menu-item menu-item-type-custom menu-item-object-custom menu-item-50171"><a href="https://www.schneier.com/talks/">Talks</a></li>
<li class="menu7 menu-item menu-item-type-post_type menu-item-object-page menu-item-50167"><a href="https://www.schneier.com/academic/">Academic</a></li>
<li class="menu8 menu-item menu-item-type-post_type menu-item-object-page menu-item-50174"><a href="https://www.schneier.com/blog/about/">About Me</a></li>
</ul></div>				</div>
			</nav>
		</footer>

		</div><!--#main-->
	</div><!--#wrapper-->

		<script type="text/javascript">
		(function () {
			var c = document.body.className;
			c = c.replace(/woocommerce-no-js/, 'woocommerce-js');
			document.body.className = c;
		})();
	</script>
	<script type='text/javascript' src='https://c0.wp.com/p/woocommerce/5.1.0/assets/js/jquery-cookie/jquery.cookie.min.js' id='jquery-cookie-js'></script>
<script type='text/javascript' src='https://149400697.v2.pressablecdn.com/wp-content/themes/schneier/assets/vendor/socialshareprivacy/js/socialshareprivacy.js?ver=1.0.1' id='social-share-privacy-js'></script>
<script type='text/javascript' id='social-share-privacy-icons-js-extra'>
/* <![CDATA[ */
var schneierSocial = {"path":"https:\/\/www.schneier.com\/wp-content\/themes\/schneier\/assets\/vendor\/socialshareprivacy\/"};
/* ]]> */
</script>
<script type='text/javascript' src='https://149400697.v2.pressablecdn.com/wp-content/themes/schneier/assets/vendor/socialshareprivacy/js/icons.js?ver=1.0.0' id='social-share-privacy-icons-js'></script>
<script type='text/javascript' id='schneier-comment-js-extra'>
/* <![CDATA[ */
var schneierComment = {"translateErrorSecurityAnswerWrong":"Your response to the challenge question ('The name of this blog is Schneier on ____') was not correct. Please try again.","ajax_url":"https:\/\/www.schneier.com\/wp-admin\/admin-ajax.php"};
/* ]]> */
</script>
<script type='text/javascript' src='https://149400697.v2.pressablecdn.com/wp-content/themes/schneier/assets/js/comment.js?ver=1.0.1' id='schneier-comment-js'></script>
<script type='text/javascript' src='https://c0.wp.com/c/5.7/wp-includes/js/wp-embed.min.js' id='wp-embed-js'></script>

</body>

</html>
